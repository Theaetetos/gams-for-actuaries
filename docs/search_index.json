[["index.html", "Generalized Additive Models for Actuaries Introduction 0.1 Our Workspace 0.2 Preliminaries", " Generalized Additive Models for Actuaries Nathan Cornwell September 28 2023 Introduction LinkedIn Email 0.1 Our Workspace R/RStudio Github Repo Github Page 0.2 Preliminaries 0.2.1 Packages Here are all of the packages we will need for this presentation. You only need to run the first code chunk once for each installation of R you have, while you need to run the second every time you start a new R session. install.packages( c(&#39;data.table&#39;, &#39;ggplot2&#39;, &#39;gratia&#39;, &#39;gridExtra&#39;, &#39;mgcv&#39;, &#39;parallel&#39;, &#39;scales&#39;) ) library(data.table) library(ggplot2) library(gratia) library(gridExtra) library(mgcv) library(parallel) library(scales) 0.2.2 One of Our Datasets We will use the diamonds dataset in the first lesson. The rest of the data is modified, masked, and randomized real data from projects Ive done. data(&#39;diamonds&#39;) 0.2.3 Branding Code Finally, this is some branding stuff that I use to make my plots look nice and comply with Optum and UHC branding guidelines. You dont need it outside of the code in the presentation, but you can use it as a template for customizing your own plots, if you wish. primary_colors &lt;- list(`Optum Orange` = &#39;#FF612B&#39;) sec_colors &lt;- list(White = &#39;#FFFFFF&#39;, `Sky Blue` = &#39;#D9F6FA&#39;, `Warm White` = &#39;#FBF9F4&#39;) type_colors &lt;- list(`Dark Blue` = &#39;#002677&#39;, `Dark Gray` = &#39;#5A5A5A&#39;) viz_colors &lt;- list(Violet = &#39;#422C88&#39;, Iris = &#39;#8061BC&#39;, Lagoon = &#39;#007C89&#39;, Rainwater = &#39;#6FC1B1&#39;, Strawberry = &#39;#A32A2E&#39;, Apple = &#39;#D13F44&#39;) state_colors &lt;- list(`Green Success` = &#39;#007000&#39;, `Gold Callout` = &#39;#F5B700&#39;, `Red Alert` = &#39;#C40000&#39;) uhc_colors &lt;- list(Blue = &#39;#002677&#39;, White = &#39;#FFFFFF&#39;, `Bright Blue` = &#39;#00BED5&#39;, Gold = &#39;#F5B700&#39;, Orange = &#39;#FF681F&#39;, `Bright Blue 20%` = &#39;CCF2F7&#39;, `Bright Blue 40%` = &#39;#99E5EE&#39;, `Dark Gray` = &#39;#5A5A5A&#39;, `Light Gray` = &#39;#F4F4F4&#39;, `Medium Gray` = &#39;#DADBDC&#39;, Hyperlink = &#39;#196ECF&#39;) colors &lt;- list(Primary = primary_colors, Secondary = sec_colors, Typography = type_colors, Visualization = viz_colors, State = state_colors, UHC = uhc_colors) scale_discrete_optum &lt;- function(aesthetics, ...){ discrete_scale( aesthetics = aesthetics, scale_name = &#39;optum&#39;, palette = grDevices::colorRampPalette( as.character(colors$Visualization[c(4, 3, 1, 2, 6, 5)]) ), ... ) } theme_titles &lt;- theme( plot.title = element_text(hjust = .5), plot.subtitle = element_text(hjust = .5) ) theme_rotx &lt;- theme(axis.text.x = element_text(angle = 90)) theme_optum_plot &lt;- theme( text = element_text(color = colors$Typography$`Dark Gray`), panel.background = element_rect( fill = colors$Secondary$White, color = colors$Typography$`Dark Gray` ) ) mytheme &lt;- theme_optum_plot + theme_titles "],["motivating-gams.html", "Chapter 1 Motivating GAMs 1.1 Linear Regression 1.2 Data 1.3 Model Fitting 1.4 Generalized Linear Models 1.5 Generalized Additive Models", " Chapter 1 Motivating GAMs 1.1 Linear Regression Well start with good ol linear regression and build up to generalized additive models. The mathematical form of linear regression is \\[ y \\sim \\beta_0+\\sum_{i=1}^n \\beta_i x_i+ \\epsilon \\\\ \\epsilon \\sim N(0,\\sigma^2) \\] In English, some response variable \\(y\\) is modeled as the sum of an intercept term \\(\\beta_0\\) and \\(n\\) predictor variables \\(x_1,...,x_n\\) with random error \\(\\epsilon \\sim N(0,\\sigma^2)\\). 1.2 Data I will skip straight to a fully-processed dataset based on other tutorials Ive given with this data. The main things Im doing are converting the categorical variables to factors (they are ordered factors in the original data for some reason) and setting the reference level to the one that occurs the most frequently in the data. As you have learned or will learn on the Predictive Analytics exam, this is general best practice. dat &lt;- as.data.table(diamonds) dat[ , clarity := relevel( factor( clarity, ordered = F ), ref = &#39;SI1&#39; ) ] dat[ , color := relevel( factor( color, ordered = F ), ref = &#39;G&#39; ) ] dat[ , cut := relevel( factor( cut, ordered = F ), ref = &#39;Ideal&#39; ) ] 1.3 Model Fitting We can fit a linear regression to our diamonds dataset as follows. I leave out the depth and table variables because, although Ive tried quite a bit, I dont understand them and dont trust their contributions to the model. Youll just have to trust me on this for now, but Id love to have someone explain how to incorporate them into a model in a rigorous way. m_lm &lt;- lm(price ~ carat + cut + color + clarity, data = dat) The first thing to do with a model is to examine the estimated coefficients. summary(m_lm) ## ## Call: ## lm(formula = price ~ carat + cut + color + clarity, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16813.5 -680.4 -197.6 466.4 10394.9 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3297.06 18.84 -175.049 &lt;2e-16 *** ## carat 8886.13 12.03 738.437 &lt;2e-16 *** ## cutFair -998.25 30.66 -32.563 &lt;2e-16 *** ## cutGood -342.49 18.52 -18.489 &lt;2e-16 *** ## cutVery Good -149.54 13.27 -11.273 &lt;2e-16 *** ## cutPremium -128.86 12.89 -9.994 &lt;2e-16 *** ## colorD 506.20 18.12 27.933 &lt;2e-16 *** ## colorE 294.52 16.20 18.183 &lt;2e-16 *** ## colorF 202.89 16.15 12.561 &lt;2e-16 *** ## colorH -472.50 16.86 -28.029 &lt;2e-16 *** ## colorI -934.10 19.36 -48.239 &lt;2e-16 *** ## colorJ -1819.02 24.84 -73.227 &lt;2e-16 *** ## clarityI1 -3573.69 44.60 -80.132 &lt;2e-16 *** ## claritySI2 -947.74 16.00 -59.224 &lt;2e-16 *** ## clarityVS2 644.14 14.64 44.006 &lt;2e-16 *** ## clarityVS1 961.19 16.53 58.137 &lt;2e-16 *** ## clarityVVS2 1393.51 19.55 71.262 &lt;2e-16 *** ## clarityVVS1 1498.34 22.25 67.341 &lt;2e-16 *** ## clarityIF 1845.96 29.86 61.827 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1157 on 53921 degrees of freedom ## Multiple R-squared: 0.9159, Adjusted R-squared: 0.9159 ## F-statistic: 3.264e+04 on 18 and 53921 DF, p-value: &lt; 2.2e-16 This readout gives us a ton of information. The first thing I like to look at is the p-values for all of the terms we included in our model. This can be found in the column labeled Pr(&gt;|t|). These are the probabilities that we would observe an effect of the size estimated by the model due to chance. We want this probability to be below some critical value \\(\\alpha\\), which is usually set to .05. If the probability is above \\(\\alpha\\), then we dont have enough evidence to include that term in our model. We call estimated coefficients with a p-value less than \\(\\alpha\\) statistically significant. The second thing I look at is the adjusted R-squared, or \\(R^2_{adj}\\). \\(R^2\\) is usually interpreted as the proportion of variance in the response variable that is explained by the model. Higher values are always better, but there is no overarching threshold that you should shoot for. Some fields of study, such as biology, psychology, and unfortunately for us, short-term actuarial science, are simply subject to very high variance, so \\(R^2\\) as low as .1 can be acceptable even for peer-reviewed studies. \\(R^2_{adj}\\) adds a penalty for each term in the model, since \\(R^2\\) always improves by at least a little whenever a term is added. Even though weve learned a lot from this readout, and all of it sounds good, we are by no means done. The next thing I like to do is plot actuals vs. predicted to see if the relationships between variables estimated by the model are tracking those in the data. preds &lt;- cbind(dat, predicted = predict(m_lm)) ggplot(preds) + geom_point(aes(price, predicted)) + geom_abline( slope = 1, intercept = 0, color = colors$Visualization$Strawberry ) + scale_x_continuous(labels = dollar) + scale_y_continuous(labels = dollar) + labs(x = &#39;Actual&#39;, y = &#39;Predicted&#39;) + mytheme This plot shows us that our model may not be so great after all. The red line shows us what a perfect model with zero residual variance would look like. Its clear that our model is significantly over-predicting the prices of very cheap and very expensive diamonds. Our coefficients may have passed the test for statistical significance, but it looks theres more going on in the data than we were able to capture. There is a lot more validation work to be done, but I am going to put that off until we get to GAMs because that is the model form were most interested in. 1.4 Generalized Linear Models Generalized linear models (GLMs) were introduced in 1972 as a formal specification of a family of models that includes linear regression as well as many others. Compared to linear regression, GLMs have relaxed constraints on the assumed error distribution and add a link function to the model form. Mathematically, a GLM is: A random variable \\(Y\\) that is conditionally distributed according to a member of the linear exponential family of distributions; A linear predictor \\(\\eta = \\sum_{i=0}^n{\\beta_iX_i}\\); and A link function \\(g\\) such that \\(E(Y|X) = g^{-1}(\\eta)\\). We dont need to bother with the mathematics of the linear exponential family. Its enough for us to note that it contains a distribution for basically any modeling problem: Bernoulli - The probability that an event will occur; e.g. a patient will develop sepsis Binomial/Negative Binomial - The expected number of occurrences out of \\(N\\) trials; e.g. the count of distinct medications a member will take in a year Poisson - The expected number of occurrences in a given amount of time/space; e.g. the number of patients visiting a doctors office in a day Normal - A numeric value on the real line You can see a full list here. Note that the last error assumption, when paired with the identity link function, is simply linear regression. You may also know the Bernoulli case as logistic regression, named for its canonical link function, the logistic function \\(\\frac{L}{1+e^{-k(x-x_0)}}\\). We will first take advantage of the GLMs ability to add a link function. Its basically always a good idea to use a log link when working with price data, if only because the range of possible predicted values is restricted to the positive reals - we almost never want to predict a negative price. m_glm &lt;- glm( price ~ carat + cut + color + clarity, family = gaussian(link = &#39;log&#39;), data = dat ) summary(m_glm) ## ## Call: ## glm(formula = price ~ carat + cut + color + clarity, family = gaussian(link = &quot;log&quot;), ## data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -75260 -1291 -695 695 12438 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.036821 0.005621 1251.79 &lt;2e-16 *** ## carat 1.397089 0.002699 517.66 &lt;2e-16 *** ## cutFair -0.237914 0.008208 -28.98 &lt;2e-16 *** ## cutGood -0.079164 0.004953 -15.98 &lt;2e-16 *** ## cutVery Good -0.003201 0.003478 -0.92 0.357 ## cutPremium -0.041107 0.003208 -12.81 &lt;2e-16 *** ## colorD 0.112976 0.005301 21.31 &lt;2e-16 *** ## colorE 0.063442 0.004702 13.49 &lt;2e-16 *** ## colorF 0.080921 0.004219 19.18 &lt;2e-16 *** ## colorH -0.152879 0.004081 -37.46 &lt;2e-16 *** ## colorI -0.305330 0.004384 -69.65 &lt;2e-16 *** ## colorJ -0.582432 0.005708 -102.05 &lt;2e-16 *** ## clarityI1 -2.340860 0.020895 -112.03 &lt;2e-16 *** ## claritySI2 -0.294332 0.003840 -76.65 &lt;2e-16 *** ## clarityVS2 0.133571 0.003718 35.93 &lt;2e-16 *** ## clarityVS1 0.234327 0.004249 55.15 &lt;2e-16 *** ## clarityVVS2 0.319816 0.005520 57.94 &lt;2e-16 *** ## clarityVVS1 0.285559 0.007516 37.99 &lt;2e-16 *** ## clarityIF 0.400077 0.009034 44.28 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 2513220) ## ## Null deviance: 8.5847e+11 on 53939 degrees of freedom ## Residual deviance: 1.3551e+11 on 53921 degrees of freedom ## AIC: 948013 ## ## Number of Fisher Scoring iterations: 8 # type argument must be specified for GLMs preds &lt;- cbind(dat, predicted = predict(m_glm, type = &#39;response&#39;)) ggplot(preds) + geom_point(aes(price, predicted)) + geom_abline( slope = 1, intercept = 0, color = colors$Visualization$Strawberry ) + scale_x_continuous(labels = dollar) + scale_y_continuous(labels = dollar) + labs(x = &#39;Actual&#39;, y = &#39;Predicted&#39;) + mytheme The bulk of our predictions are now centered around the red line, but the problem of predictions being too high for more expensive diamonds is worse. In addition, there is a region of data points below the main region. This suggests that the model is missing some relationship in the data. ggplot(preds) + geom_point( aes( price, predicted, # I do this just to make the legend a little more readable color = factor( clarity, levels = rev( c(&#39;I1&#39;, &#39;SI2&#39;, &#39;SI1&#39;, &#39;VS2&#39;, &#39;VS1&#39;, &#39;VVS2&#39;, &#39;VVS1&#39;, &#39;IF&#39;) ) ) ) ) + scale_discrete_optum(&#39;color&#39;) + scale_x_continuous(labels = dollar) + scale_y_continuous(labels = dollar) + labs(x = &#39;Actual&#39;, y = &#39;Predicted&#39;, color = element_blank()) + mytheme This residual plot makes the cause of the grouping issue very clear: the price slope for diamonds of clarity IF looks to be quite different from the slopes of the rest of the clarity ratings. Based on the groupings visible in the above plot, we need to allow the price slope to vary by clarity. The other main issue is not as clear, but I will skip straight to it to save us time: the price-carat relationship also needs to be able to change slope. We can implement these changes with the below code. m_glm2 &lt;- glm( price ~ poly(carat, 3)*clarity + cut + color, data = dat, family = gaussian(link = &#39;log&#39;) ) summary(m_glm2) ## ## Call: ## glm(formula = price ~ poly(carat, 3) * clarity + cut + color, ## family = gaussian(link = &quot;log&quot;), data = dat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -10624.8 -200.5 -26.5 180.3 7096.0 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.768288 0.004583 1695.100 &lt; 2e-16 *** ## poly(carat, 3)1 231.821770 0.913825 253.683 &lt; 2e-16 *** ## poly(carat, 3)2 -54.434208 0.430228 -126.524 &lt; 2e-16 *** ## poly(carat, 3)3 17.669019 0.659020 26.811 &lt; 2e-16 *** ## clarityI1 -0.264844 0.020736 -12.772 &lt; 2e-16 *** ## claritySI2 -0.068189 0.007285 -9.361 &lt; 2e-16 *** ## clarityVS2 0.134356 0.005638 23.829 &lt; 2e-16 *** ## clarityVS1 0.187955 0.006317 29.753 &lt; 2e-16 *** ## clarityVVS2 0.291332 0.006866 42.433 &lt; 2e-16 *** ## clarityVVS1 0.369897 0.007156 51.693 &lt; 2e-16 *** ## clarityIF 0.469748 0.008872 52.946 &lt; 2e-16 *** ## cutFair -0.166446 0.003241 -51.359 &lt; 2e-16 *** ## cutGood -0.077725 0.002042 -38.054 &lt; 2e-16 *** ## cutVery Good -0.034547 0.001440 -23.995 &lt; 2e-16 *** ## cutPremium -0.049561 0.001314 -37.726 &lt; 2e-16 *** ## colorD 0.174176 0.002176 80.044 &lt; 2e-16 *** ## colorE 0.128012 0.001929 66.357 &lt; 2e-16 *** ## colorF 0.090840 0.001746 52.032 &lt; 2e-16 *** ## colorH -0.109804 0.001701 -64.550 &lt; 2e-16 *** ## colorI -0.200778 0.001837 -109.303 &lt; 2e-16 *** ## colorJ -0.348985 0.002333 -149.583 &lt; 2e-16 *** ## poly(carat, 3)1:clarityI1 -68.202418 3.282812 -20.776 &lt; 2e-16 *** ## poly(carat, 3)2:clarityI1 22.986484 1.265368 18.166 &lt; 2e-16 *** ## poly(carat, 3)3:clarityI1 -11.706733 0.725728 -16.131 &lt; 2e-16 *** ## poly(carat, 3)1:claritySI2 -25.448899 1.437627 -17.702 &lt; 2e-16 *** ## poly(carat, 3)2:claritySI2 12.372250 0.705059 17.548 &lt; 2e-16 *** ## poly(carat, 3)3:claritySI2 -14.280810 0.782883 -18.241 &lt; 2e-16 *** ## poly(carat, 3)1:clarityVS2 1.870011 1.125183 1.662 0.096526 . ## poly(carat, 3)2:clarityVS2 -7.297582 0.575906 -12.671 &lt; 2e-16 *** ## poly(carat, 3)3:clarityVS2 -1.254099 0.737984 -1.699 0.089257 . ## poly(carat, 3)1:clarityVS1 8.735755 1.319027 6.623 3.56e-11 *** ## poly(carat, 3)2:clarityVS1 -10.865925 0.647157 -16.790 &lt; 2e-16 *** ## poly(carat, 3)3:clarityVS1 9.321429 1.069932 8.712 &lt; 2e-16 *** ## poly(carat, 3)1:clarityVVS2 27.565527 1.790833 15.393 &lt; 2e-16 *** ## poly(carat, 3)2:clarityVVS2 -4.731728 1.433310 -3.301 0.000963 *** ## poly(carat, 3)3:clarityVVS2 54.218689 2.263802 23.950 &lt; 2e-16 *** ## poly(carat, 3)1:clarityVVS1 15.893363 1.679070 9.466 &lt; 2e-16 *** ## poly(carat, 3)2:clarityVVS1 -16.881147 1.315091 -12.836 &lt; 2e-16 *** ## poly(carat, 3)3:clarityVVS1 39.985118 2.186750 18.285 &lt; 2e-16 *** ## poly(carat, 3)1:clarityIF 17.951187 1.938529 9.260 &lt; 2e-16 *** ## poly(carat, 3)2:clarityIF -13.961946 1.311702 -10.644 &lt; 2e-16 *** ## poly(carat, 3)3:clarityIF 67.110841 2.414828 27.791 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 458620.2) ## ## Null deviance: 8.5847e+11 on 53939 degrees of freedom ## Residual deviance: 2.4719e+10 on 53898 degrees of freedom ## AIC: 856280 ## ## Number of Fisher Scoring iterations: 5 preds &lt;- cbind(dat, predicted = predict(m_glm2, type = &#39;response&#39;)) ggplot(preds) + geom_point(aes(price, predicted)) + geom_abline( slope = 1, intercept = 0, color = colors$Visualization$Strawberry ) + scale_x_continuous(labels = dollar) + scale_y_continuous(labels = dollar) + labs(x = &#39;Actual&#39;, y = &#39;Predicted&#39;) + mytheme This now looks good enough to get on with. We can visualize our fancy new price slopes in the following chart. # CJ stands for &#39;Cross Join&#39;, and creates a data.table that is the cartesian product of all of the input vectors pdat &lt;- CJ( # we need to apply the link function to any numeric variables carat = log(seq(from = .01, to = 5.01, by = .01)), clarity = factor( levels(dat$clarity), levels = c(&#39;IF&#39;, &#39;VVS1&#39;, &#39;VVS2&#39;, &#39;VS1&#39;, &#39;VS2&#39;, &#39;SI1&#39;, &#39;SI2&#39;, &#39;I1&#39;) ), cut = factor(&#39;Ideal&#39;, levels = levels(dat$cut)), color = factor(&#39;G&#39;, levels = levels(dat$color)) ) pdat[ # setting type = &#39;response&#39; applies the inverse link function to the linear predictor for us , pred := predict(m_glm2, newdata = pdat, type = &#39;response&#39;) ] ggplot(pdat, aes(x = exp(carat), group = clarity, color = clarity)) + geom_line(aes(y = pred)) + scale_discrete_optum(aesthetics = &#39;color&#39;) + scale_y_continuous(labels = dollar) + labs( title = &#39;Visualization of Price Slopes&#39;, x = &#39;Carat&#39;, y = &#39;Price&#39;, color = &#39;Clarity&#39; ) + mytheme 1.5 Generalized Additive Models Generalized Additive Models (GAMs) were introduced in the 90s by Simon Wood as an extension of GLMs that incorporate a form of functional regression. They have all of the same characteristics of GLMs in addition to the ability to model functional, not just linear, relationships between predictor variables and the response, as well as an expanded field of error distributions, most notably for actuarial work the Tweedie and Negative Binomial. Functional relationships are modeled by GAMs using various types of splines, which are piecewise functions that are used to interpolate a continuously-differentiable curve between a collection of points. The linear predictor for a GAM, therefore, looks like this: \\[\\eta = \\sum_{i=0}^n\\sum_{j=0}^m\\beta_{ij}f_j(X_i)\\] With the added dimension \\(j\\) denoting the basis dimension, or each piece of the piecewise spline \\(f_j(X_i)\\). This sounds a lot scarier than it is; it should become clear how splines work in GAMs once we fit one to our diamonds dataset. GAMs are fit a lot like GLMs. The function to use is called gam(), and it takes the same arguments formula, data, and family as glm(). You specify a functional term by wrapping it in s() or one of the other smooth constructors offered in the mgcv package. Here, I am using the ti() (short for tensor interaction) constructor because this is the recommended way to create interaction terms in the context of GAMs. I also parallelize model fitting, because otherwise fitting takes hours on my 32G work laptop (!). The bam() function is just short for big gam and includes a new argument, cluster, which is a parallel computing cluster created using the parallel package. It is recommended to fit GAMs in parallel if the number of rows in your training dataset is five figures or more. cluster &lt;- makePSOCKcluster(detectCores()) m_gam &lt;- bam( price ~ ti(carat) + clarity + ti(carat, by = clarity) + cut + color, data = dat, family = gaussian(link = &#39;log&#39;), cluster = cluster ) # you must always stop the cluster you create stopCluster(cluster) summary(m_gam) ## ## Family: gaussian ## Link function: log ## ## Formula: ## price ~ ti(carat) + clarity + ti(carat, by = clarity) + cut + ## color ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.709623 0.005248 1468.977 &lt;2e-16 *** ## clarityI1 -0.469449 0.051876 -9.049 &lt;2e-16 *** ## claritySI2 -0.148794 0.010080 -14.762 &lt;2e-16 *** ## clarityVS2 0.147028 0.006516 22.563 &lt;2e-16 *** ## clarityVS1 0.213806 0.007026 30.430 &lt;2e-16 *** ## clarityVVS2 0.341344 0.007509 45.460 &lt;2e-16 *** ## clarityVVS1 0.417397 0.007547 55.308 &lt;2e-16 *** ## clarityIF 0.514192 0.009151 56.189 &lt;2e-16 *** ## cutFair -0.169132 0.003161 -53.510 &lt;2e-16 *** ## cutGood -0.080538 0.001996 -40.359 &lt;2e-16 *** ## cutVery Good -0.036635 0.001407 -26.043 &lt;2e-16 *** ## cutPremium -0.049751 0.001282 -38.805 &lt;2e-16 *** ## colorD 0.176630 0.002124 83.155 &lt;2e-16 *** ## colorE 0.130101 0.001883 69.087 &lt;2e-16 *** ## colorF 0.091305 0.001704 53.578 &lt;2e-16 *** ## colorH -0.111324 0.001661 -67.014 &lt;2e-16 *** ## colorI -0.204098 0.001800 -113.366 &lt;2e-16 *** ## colorJ -0.349960 0.002283 -153.310 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## ti(carat) 3.9969 4.000 6450.26 &lt; 2e-16 *** ## ti(carat):claritySI1 3.9573 3.999 61.37 &lt; 2e-16 *** ## ti(carat):clarityI1 3.7512 3.944 117.79 &lt; 2e-16 *** ## ti(carat):claritySI2 3.9663 3.999 172.61 &lt; 2e-16 *** ## ti(carat):clarityVS2 0.9998 1.001 11.51 0.000692 *** ## ti(carat):clarityVS1 3.9317 3.997 21.21 &lt; 2e-16 *** ## ti(carat):clarityVVS2 3.8454 3.977 99.29 &lt; 2e-16 *** ## ti(carat):clarityVVS1 3.8206 3.971 82.44 &lt; 2e-16 *** ## ti(carat):clarityIF 2.9045 2.991 209.77 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Rank: 53/54 ## R-sq.(adj) = 0.973 Deviance explained = 97.3% ## fREML = 4.2696e+05 Scale est. = 4.3652e+05 n = 53940 Lets compare these price slopes to those from our original GLM fit. pdat[ , `:=`( gam = predict(m_gam, newdata = pdat, type = &#39;response&#39;), glm = pred ) ] ggplot(pdat, aes(x = exp(carat), group = clarity, color = clarity)) + geom_line(aes(y = glm), linetype = &#39;dashed&#39;) + geom_line(aes(y = gam)) + scale_discrete_optum(aesthetics = &#39;color&#39;) + scale_y_continuous(labels = dollar) + labs( title = &#39;Visualization of Price Slopes&#39;, x = &#39;Carat&#39;, y = &#39;Price&#39;, color = &#39;Clarity&#39; ) + mytheme Here, we can see that the polynomial relationship between carat and price we modeled with a GLM is picked up automatically by the GAM. "],["utilization-forecasting.html", "Chapter 2 Utilization Forecasting 2.1 Introduction 2.2 Data 2.3 Predictors 2.4 Model Fitting 2.5 Making Predictions 2.6 Alternative Trend Forecast", " Chapter 2 Utilization Forecasting 2.1 Introduction One of the key use cases for GAMs in my work is in utilization forecasting. In this case study, I will walk through how I do so using a particular example drug. 2.2 Data The data we will use is a table of daily utilization according to an unspecified metric for an unspecified drug. dat &lt;- fread( file = file.path(&#39;data&#39;, &#39;rx_data.csv&#39;), colClasses = c(ds = &#39;Date&#39;) ) ggplot(dat) + geom_line(aes(ds, y)) + labs( x = element_blank(), y = &#39;Utilization&#39; ) + mytheme 2.3 Predictors 2.3.1 Weekday The first and largest source of variation is due to the day of the week. dat[ , wkdy := factor( strftime(ds, &#39;%u&#39;), levels = as.character(1:7), labels = c(&#39;M&#39;, &#39;T&#39;, &#39;W&#39;, &#39;Tr&#39;, &#39;F&#39;, &#39;Sat&#39;, &#39;Sun&#39;) ) ] ggplot(dat) + geom_boxplot(aes(wkdy, y)) + labs( x = element_blank(), y = &#39;Utilization&#39; ) + mytheme 2.3.2 In-year The next source of variation is the time of year. A phenomenon called induced utilization is almost always present in health claims, where members are more likely to seek care later in the year as their deductible gets used up. Some drugs display other effects, as well, like flu treatments, which spike in Winter, and immunizations, which spike in Fall before the school year starts. dat[ , yr := year(ds) ][ , yrfrac := as.numeric(strftime(ds, &#39;%j&#39;)) ][ , yrfrac := yrfrac / max(yrfrac) ] ggplot(dat, aes(yrfrac, y)) + geom_hline(yintercept = 90, linetype = &#39;dashed&#39;) + geom_point() + geom_smooth() + labs( x = &#39;Proportion of Year Completed&#39;, y = &#39;Utilization&#39; ) + mytheme ## `geom_smooth()` using method = &#39;gam&#39; and formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; 2.3.3 Holidays We will cover a more rigorous method for selecting a holiday list in the next section. For now, we will just use the big six. Just using this holiday list usually will get you most of the way there. With some insured populations, however, days outside the list of U.S. federal holidays need to be accounted for. In addition, billing practices can cause spikes on days like the first of the month. lbls &lt;- c(paste0(12, 16:31), paste0(&#39;01&#39;, 0, 1:9)) pdat &lt;- dat[ , .(day = strftime(ds, &#39;%m%d&#39;), yr = year(ds) + (month(ds) == 12), wkdy, y) ][ .(lbls), on = &#39;day&#39; ][ , `:=`( yr = factor(yr), day = factor(day, levels = lbls) ) ] ggplot(pdat) + geom_bar( aes(day, y, group = wkdy, fill = wkdy), stat = &#39;identity&#39; ) + facet_grid(yr ~.) + scale_discrete_optum(&#39;fill&#39;) + labs( title = &#39;Christmas and New Year\\&#39;s&#39;, x = element_blank(), y = &#39;Utilization&#39;, fill = &#39;Weekday&#39; ) + mytheme + theme_rotx ny &lt;- dat[month(ds) == 1 &amp; mday(ds) == 1, .(ds, hol = &#39;New Year&#39;)] mem &lt;- dat[ month(ds) == 5 &amp; as.character(wkdy) == &#39;M&#39;, .(ds = tail(ds, 1), hol = &#39;Memorial Day&#39;), by = .(yr) ][ , yr := NULL ] indep &lt;- dat[ month(ds) == 7 &amp; mday(ds) == 4, .(ds, hol = &#39;Independence Day&#39;)] labor &lt;- dat[ month(ds) == 9 &amp; as.character(wkdy) == &#39;M&#39;, .(ds = head(ds, 1), hol = &#39;Labor Day&#39;), by = .(yr) ][ , yr := NULL ] thnks &lt;- dat[ month(ds) == 11 &amp; as.character(wkdy) == &#39;Tr&#39; ][ , i := seq_len(.N), by = .(yr) ][ i == 4, .(ds, hol = &#39;Thanksgiving&#39;) ] xmas &lt;- dat[ month(ds) == 12 &amp; mday(ds) == 25, .(ds, hol = &#39;Christmas&#39;) ] hols &lt;- rbind( ny, mem, indep, labor, thnks, xmas ) dat &lt;- hols[dat, on = &#39;ds&#39;] dat[is.na(hol), hol := &#39;none&#39;] dat[, hol := relevel(factor(hol), ref = &#39;none&#39;)] dat[] ## ds hol y wkdy yr yrfrac ## 1: 2015-01-01 New Year 20.35787 Tr 2015 0.002732240 ## 2: 2015-01-02 none 49.75367 F 2015 0.005464481 ## 3: 2015-01-03 none 28.11368 Sat 2015 0.008196721 ## 4: 2015-01-04 none 30.62088 Sun 2015 0.010928962 ## 5: 2015-01-05 none 83.43423 M 2015 0.013661202 ## --- ## 2949: 2023-01-27 none 132.02780 F 2023 0.073770492 ## 2950: 2023-01-28 none 74.57140 Sat 2023 0.076502732 ## 2951: 2023-01-29 none 69.18019 Sun 2023 0.079234973 ## 2952: 2023-01-30 none 175.67447 M 2023 0.081967213 ## 2953: 2023-01-31 none 156.08502 T 2023 0.084699454 2.3.4 COVID The most common impact to the Pharmacy line of business is actually a large spike in utilization at the start of lockdowns. This occurred at UHC, at least, because we removed several restrictions designed to prevent stockpiling and other forms of overutilization so that members could stock up on the medicines they need in the face of the very uncertain short-term future at the time. ggplot(dat) + geom_line(aes(ds, y)) + coord_cartesian(xlim = as.Date(c(&#39;2020-3-1&#39;, &#39;2020-3-31&#39;))) + labs( x = element_blank(), y = &#39;Utilization&#39; ) + mytheme dat[, covid := 0L] dat[ strftime(ds, &#39;%Y%m&#39;) == &#39;202003&#39;, covid := .I ] dat[] ## ds hol y wkdy yr yrfrac covid ## 1: 2015-01-01 New Year 20.35787 Tr 2015 0.002732240 0 ## 2: 2015-01-02 none 49.75367 F 2015 0.005464481 0 ## 3: 2015-01-03 none 28.11368 Sat 2015 0.008196721 0 ## 4: 2015-01-04 none 30.62088 Sun 2015 0.010928962 0 ## 5: 2015-01-05 none 83.43423 M 2015 0.013661202 0 ## --- ## 2949: 2023-01-27 none 132.02780 F 2023 0.073770492 0 ## 2950: 2023-01-28 none 74.57140 Sat 2023 0.076502732 0 ## 2951: 2023-01-29 none 69.18019 Sun 2023 0.079234973 0 ## 2952: 2023-01-30 none 175.67447 M 2023 0.081967213 0 ## 2953: 2023-01-31 none 156.08502 T 2023 0.084699454 0 2.3.5 Trend dat[, t := .I] dat[] ## ds hol y wkdy yr yrfrac covid t ## 1: 2015-01-01 New Year 20.35787 Tr 2015 0.002732240 0 1 ## 2: 2015-01-02 none 49.75367 F 2015 0.005464481 0 2 ## 3: 2015-01-03 none 28.11368 Sat 2015 0.008196721 0 3 ## 4: 2015-01-04 none 30.62088 Sun 2015 0.010928962 0 4 ## 5: 2015-01-05 none 83.43423 M 2015 0.013661202 0 5 ## --- ## 2949: 2023-01-27 none 132.02780 F 2023 0.073770492 0 2949 ## 2950: 2023-01-28 none 74.57140 Sat 2023 0.076502732 0 2950 ## 2951: 2023-01-29 none 69.18019 Sun 2023 0.079234973 0 2951 ## 2952: 2023-01-30 none 175.67447 M 2023 0.081967213 0 2952 ## 2953: 2023-01-31 none 156.08502 T 2023 0.084699454 0 2953 2.4 Model Fitting m &lt;- gam( y ~ s(t) + s(yrfrac) + s(covid) + wkdy + hol, family = gaussian(link = &#39;log&#39;), data = dat ) summary(m) ## ## Family: gaussian ## Link function: log ## ## Formula: ## y ~ s(t) + s(yrfrac) + s(covid) + wkdy + hol ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.759475 0.002884 1650.54 &lt;2e-16 *** ## wkdyT -0.109329 0.004116 -26.56 &lt;2e-16 *** ## wkdyW -0.144918 0.004204 -34.47 &lt;2e-16 *** ## wkdyTr -0.183170 0.004319 -42.41 &lt;2e-16 *** ## wkdyF -0.274914 0.004542 -60.52 &lt;2e-16 *** ## wkdySat -0.785000 0.006598 -118.98 &lt;2e-16 *** ## wkdySun -0.842564 0.006926 -121.65 &lt;2e-16 *** ## holChristmas -1.365148 0.110336 -12.37 &lt;2e-16 *** ## holIndependence Day -0.945942 0.065771 -14.38 &lt;2e-16 *** ## holLabor Day -0.948133 0.050228 -18.88 &lt;2e-16 *** ## holMemorial Day -0.968767 0.051515 -18.81 &lt;2e-16 *** ## holNew Year -0.890186 0.064132 -13.88 &lt;2e-16 *** ## holThanksgiving -1.276025 0.080959 -15.76 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(t) 6.053 7.191 4110.09 &lt;2e-16 *** ## s(yrfrac) 8.916 8.998 14.34 &lt;2e-16 *** ## s(covid) 6.367 7.450 14.98 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.962 Deviance explained = 96.2% ## GCV = 48.978 Scale est. = 48.408 n = 2953 plot(m, pages = 1) Although the in-year term is statistically significant, it is not a likely shape, so I am concerned that it is overfitting the training data. Right now, we have the luxury of trying out alternative model forms, but in production I would likely just leave it alone because it will contribute very little to forecasts. The help page for s() (?s in the console) directs us to a help page for smooth.terms, which contains a list of different options to use. We see that by specifying bs = 'ts', we can apply a stricter penalty to the spline to yield a smootherpotentially even nullestimate. m &lt;- gam( y ~ s(t) + s(yrfrac, bs = &#39;ts&#39;) + s(covid) + wkdy + hol, family = gaussian(link = &#39;log&#39;), data = dat ) summary(m) ## ## Family: gaussian ## Link function: log ## ## Formula: ## y ~ s(t) + s(yrfrac, bs = &quot;ts&quot;) + s(covid) + wkdy + hol ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.759475 0.002884 1650.54 &lt;2e-16 *** ## wkdyT -0.109329 0.004116 -26.56 &lt;2e-16 *** ## wkdyW -0.144918 0.004204 -34.47 &lt;2e-16 *** ## wkdyTr -0.183170 0.004319 -42.41 &lt;2e-16 *** ## wkdyF -0.274914 0.004542 -60.52 &lt;2e-16 *** ## wkdySat -0.785000 0.006598 -118.98 &lt;2e-16 *** ## wkdySun -0.842564 0.006926 -121.65 &lt;2e-16 *** ## holChristmas -1.365150 0.110336 -12.37 &lt;2e-16 *** ## holIndependence Day -0.945942 0.065771 -14.38 &lt;2e-16 *** ## holLabor Day -0.948133 0.050228 -18.88 &lt;2e-16 *** ## holMemorial Day -0.968767 0.051515 -18.81 &lt;2e-16 *** ## holNew Year -0.890185 0.064132 -13.88 &lt;2e-16 *** ## holThanksgiving -1.276024 0.080959 -15.76 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(t) 6.053 7.191 4110.09 &lt;2e-16 *** ## s(yrfrac) 8.916 9.000 14.32 &lt;2e-16 *** ## s(covid) 6.367 7.450 14.98 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.962 Deviance explained = 96.2% ## GCV = 48.978 Scale est. = 48.408 n = 2953 plot(m, pages = 1) It turns out that the estimate is robust to the extra penalty. Perhaps there is some residual month-specific variation that the weekday and holiday components arent capturing by themselves. It could also be that overall trend is masking the true in-year variation. Well leave it alone but try another way to forecast this drug a little later. 2.5 Making Predictions # all dates we want predictions for testset &lt;- data.table( ds = seq(from = dat[, min(ds)], to = as.Date(&#39;2024-12-31&#39;), by = 1) ) # add historicals testset &lt;- dat[, .(ds, y)][testset, on = &#39;ds&#39;] # redo date processing testset[ , `:=`( t = .I, yr = year(ds), yrfrac = as.numeric(strftime(ds, &#39;%j&#39;)), covid = 0L, wkdy = factor( strftime(ds, &#39;%u&#39;), labels = c(&#39;M&#39;, &#39;T&#39;, &#39;W&#39; ,&#39;Tr&#39;, &#39;F&#39;, &#39;Sat&#39;, &#39;Sun&#39;) ) ) ][ , yrfrac := yrfrac / max(yrfrac), by = .(yr) ][ strftime(ds, &#39;%Y%m&#39;) == &#39;202003&#39;, covid := .I ] # holidays ## there are more efficient ways to do this, but they required more advanced coding outside the ## scope of this module hols &lt;- rbind( hols, data.table(ds = as.Date(&#39;2024-1-1&#39;), hol = &#39;New Year&#39;), data.table( ds = as.Date(c(&#39;2023-5-29&#39;, &#39;2024-5-27&#39;)), hol = &#39;Memorial Day&#39; ), data.table( ds = as.Date(c(&#39;2023-7-4&#39;, &#39;2024-7-4&#39;)), hol = &#39;Independence Day&#39; ), data.table( ds = as.Date(c(&#39;2023-9-4&#39;, &#39;2024-9-2&#39;)), hol = &#39;Labor Day&#39; ), data.table( ds = as.Date(c(&#39;2023-11-23&#39;, &#39;2024-11-28&#39;)), hol = &#39;Thanksgiving&#39; ), data.table( ds = as.Date(c(&#39;2023-12-25&#39;, &#39;2024-12-25&#39;)), hol = &#39;Christmas&#39; ) ) testset &lt;- hols[testset, on = &#39;ds&#39;] testset[is.na(hol), hol := &#39;none&#39;] testset[, hol := relevel(factor(hol), ref = &#39;none&#39;)] # predictions testset[ , yhat := as.vector(predict(m, newdata = testset, type = &#39;response&#39;)) ] pdat &lt;- testset[ , lapply(.SD, sum), by = .(yrmo = strftime(ds, &#39;%Y%m&#39;)), .SDcols = c(&#39;y&#39;, &#39;yhat&#39;) ] ggplot(pdat, aes(x = factor(yrmo))) + geom_point(aes(y = y)) + geom_point(aes(y = yhat), color = colors$Visualization$Strawberry) + labs( &#39;Actual vs. Predicted&#39;, x = &#39;Month&#39;, y = &#39;Annualized Utilization PMPM&#39; ) + mytheme + theme_rotx ## Warning: Removed 23 rows containing missing values (`geom_point()`). Splines make predictions outside of the range of the training data by means of linear extrapolation using the first derivative of the estimated smooth at the endpoint. Since we use a log link, this means that trend is assumed to be exponential, which can cause problems if the trend slope at the end of the training data is too large. pdat &lt;- cbind( testset[, .(ds)], t = exp( as.vector( predict( m, newdata = testset, type = &#39;terms&#39;, terms = &#39;s(t)&#39; ) ) ) )[ , grp := ifelse(ds &lt; as.Date(&#39;2023-2-1&#39;), &#39;Historical&#39;, &#39;Forecasted&#39;) ] ggplot(pdat) + geom_line(aes(ds, t, group = 1, color = grp)) + scale_discrete_optum(&#39;color&#39;) + labs( title = &#39;Modeled Trend Component&#39;, x = element_blank(), y = element_blank(), color = element_blank() ) + mytheme Ours looks fine, however. In addition to this consideration, the linear extrapolation method can also be problematic if the model fits a wiggly smooth to the data. The estimated slope at the end of the training period could be very different from the overall trend. To guard against this possibility, I like to prevent the model from using any knot points in the last 80% of the training period. You can pass a list of knot points for the model to use for a particular smooth like below. # I like to allow trend to change every half year ts &lt;- dat[month(ds) %in% c(1, 7) &amp; mday(ds) == 1, t] # remove knot points outside of the allowed range ts &lt;- ts[ts &lt; .8 * dat[, max(t)]] # add back the last day counter ts &lt;- c(ts, dat[, max(t)]) m &lt;- gam( y ~ s(t) + s(yrfrac, bs = &#39;ts&#39;) + s(covid) + wkdy + hol, family = gaussian(link = &#39;log&#39;), data = dat, knots = list(t = ts) ) plot(m, select = 1) pdat &lt;- cbind( testset[, .(ds)], t = exp( as.vector( predict( m, newdata = testset, type = &#39;terms&#39;, terms = &#39;s(t)&#39; ) ) ) )[ , grp := ifelse(ds &lt; as.Date(&#39;2023-2-1&#39;), &#39;Historical&#39;, &#39;Forecasted&#39;) ] ggplot(pdat) + geom_line(aes(ds, t, group = 1, color = grp)) + scale_discrete_optum(&#39;color&#39;) + labs( title = &#39;Modeled Trend Component&#39;, x = element_blank(), y = element_blank(), color = element_blank() ) + mytheme It doesnt make much of a difference, if any, with this drug because the trend component is quite regular. 2.6 Alternative Trend Forecast Well try modeled core trend in another way to see if that improves the in-year component, and just for personal interest. This method is more in accordance with classic actuarial forecasting methods. In this framework, future trend is estimated as some sort of average of historical trends. We can estimate year-over-year trend for each year in historicals by using a dummy variable corresponding to the year. To fit a model, we just need to turn the yr variable into a factor and change the formula accordingly. 2.6.1 Model Fitting dat[, yr := factor(yr)] m &lt;- gam( y ~ yr + s(yrfrac, bs = &#39;ts&#39;) + s(covid) + wkdy + hol, family = gaussian(link = &#39;log&#39;), data = dat ) summary(m) ## ## Family: gaussian ## Link function: log ## ## Formula: ## y ~ yr + s(yrfrac, bs = &quot;ts&quot;) + s(covid) + wkdy + hol ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.330717 0.006733 643.20 &lt;2e-16 *** ## yr2016 0.119264 0.008381 14.23 &lt;2e-16 *** ## yr2017 0.255153 0.007936 32.15 &lt;2e-16 *** ## yr2018 0.400342 0.007548 53.04 &lt;2e-16 *** ## yr2019 0.510325 0.007312 69.79 &lt;2e-16 *** ## yr2020 0.617541 0.007196 85.81 &lt;2e-16 *** ## yr2021 0.714179 0.006977 102.36 &lt;2e-16 *** ## yr2022 0.779075 0.006897 112.96 &lt;2e-16 *** ## yr2023 0.858760 0.012225 70.25 &lt;2e-16 *** ## wkdyT -0.109370 0.004145 -26.38 &lt;2e-16 *** ## wkdyW -0.145014 0.004235 -34.24 &lt;2e-16 *** ## wkdyTr -0.183225 0.004350 -42.12 &lt;2e-16 *** ## wkdyF -0.275055 0.004575 -60.12 &lt;2e-16 *** ## wkdySat -0.785103 0.006646 -118.13 &lt;2e-16 *** ## wkdySun -0.842488 0.006976 -120.76 &lt;2e-16 *** ## holChristmas -1.353773 0.110659 -12.23 &lt;2e-16 *** ## holIndependence Day -0.945311 0.066230 -14.27 &lt;2e-16 *** ## holLabor Day -0.948453 0.050598 -18.75 &lt;2e-16 *** ## holMemorial Day -0.968776 0.051878 -18.67 &lt;2e-16 *** ## holNew Year -0.895965 0.064755 -13.84 &lt;2e-16 *** ## holThanksgiving -1.274799 0.081439 -15.65 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(yrfrac) 8.920 9.000 55.97 &lt;2e-16 *** ## s(covid) 6.301 7.385 14.59 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.961 Deviance explained = 96.2% ## GCV = 49.716 Scale est. = 49.106 n = 2953 plot(m, pages = 1) Some of the slope in the data is now attributed to in-year variation, which we may or may not want depending on the application. 2.6.2 Making Predictions Since the model wont know what to do with years that are beyond the last year in the training data, we have to start by transforming that variable in the forecasting dataset accordingly. testset2 &lt;- copy(testset) testset2[, yr := factor(pmin(yr, 2023))] We can convert the trend estimates for each year to annual trends for analysis like so. trnds &lt;- cbind( dat[, .(ds)], fctr = exp(as.vector(predict(m, type = &#39;terms&#39;, terms = &#39;yr&#39;))) )[ , .(fctr = mean(fctr)), by = .(yr = year(ds)) ][ , trnd := fctr^(1 / (.I - 1)) - 1 ][ yr &gt; min(yr), ] trnds[] ## yr fctr trnd ## 1: 2016 1.126667 0.1266674 ## 2: 2017 1.290659 0.1360717 ## 3: 2018 1.492336 0.1427612 ## 4: 2019 1.665832 0.1360771 ## 5: 2020 1.854362 0.1314592 ## 6: 2021 2.042509 0.1264035 ## 7: 2022 2.179455 0.1177262 ## 8: 2023 2.360233 0.1133183 Now we can use whatever method we like to calculate a future trend assumption. I will just use a weighted average by the number of months completed in each year, but we could easily imagine situations in which we may want to recognize the steady decrease in trend starting in 2019 and use some sort of extrapolation, instead. i &lt;- trnds[ , wt := 1 ][ .N, wt := 1/12 ][ , sum(trnd * wt) / sum(wt) ] i ## [1] 0.1308155 I will only use our estimated trend in 2024 to avoid a lot of tedious number-crunching to avoid a disconnect between the 2023 actuals and forecasts. testset2[ , yhat := as.vector(predict(m, newdata = testset2, type = &#39;response&#39;)) ][ year(ds) == 2024, yhat := yhat * (1 + i) ] pdat &lt;- testset2[ , lapply(.SD, sum), by = .(yrmo = strftime(ds, &#39;%Y%m&#39;)), .SDcols = c(&#39;y&#39;, &#39;yhat&#39;) ][ testset[ , .(Smooth = sum(yhat)), by = .(yrmo = strftime(ds, &#39;%Y%m&#39;)) ], on = &#39;yrmo&#39; ] setnames(pdat, old = &#39;yhat&#39;, new = &#39;Annual&#39;) pdat &lt;- melt(pdat, id.vars = c(&#39;yrmo&#39;, &#39;y&#39;)) ggplot(pdat, aes(x = factor(yrmo))) + geom_line(aes(y = y, group = variable)) + geom_point(aes(y = value, color = variable)) + scale_discrete_optum(&#39;color&#39;) + labs( &#39;Actual vs. Predicted&#39;, x = &#39;Month&#39;, y = &#39;Annualized Utilization PMPM&#39;, color = &#39;Model&#39; ) + mytheme + theme_rotx ## Warning: Removed 46 rows containing missing values (`geom_line()`). "],["hurricane-impact-valuation.html", "Chapter 3 Hurricane Impact Valuation 3.1 Introduction 3.2 Data 3.3 Model Fitting 3.4 Using the Model", " Chapter 3 Hurricane Impact Valuation 3.1 Introduction Another very similar way I use GAMs is to value utilization impacts caused by storms. The main impacts a health insurer sees from something like a hurricane is a slowdown in utilization from the danger and difficulty traveling caused by the storm. We can adapt the tools we already have from the previous chapter and add elements in order to strengthen our model to be able to use it for hyptothesis testing and quantification. In order to model the hurricanes effects as rigorously as possible, we need to do some more thorough model validation so we can make sound statistical deductions from it. 3.2 Data The data this time is utilization for a random set of services for UHCs commercial fully-insured book of business in Texas. dat &lt;- fread( file = file.path(&#39;data&#39;, &#39;hurr_data.csv&#39;), colClasses = c(ds = &#39;Date&#39;) ) ggplot(dat) + geom_line(aes(ds, y)) + labs( x = element_blank(), y = &#39;Utilization&#39; ) + mytheme The COVID slowdown is clearly visible. Note also that this data has runout effects at the end of the training period, i.e. the most recent time periods have had less times for claims to process. As you may remember, Texas was hit by the historic Hurricane Harvey in 2017. landfall &lt;- as.Date(&#39;2017-8-25&#39;) ggplot(dat) + geom_vline( xintercept = landfall, color = colors$Visualization$Lagoon, linetype = &#39;dashed&#39; ) + geom_line(aes(ds, y)) + coord_cartesian(xlim = as.Date(c(&#39;2017-8-1&#39;, &#39;2017-9-30&#39;))) + labs( x = element_blank(), y = &#39;Utilization&#39; ) + mytheme 3.3 Model Fitting 3.3.1 Initial Predictors First we add the date variables weve been discussing. We will use an initial holiday list and iteratively improve it. For the COVID term, we will just use the parameters Ive already determined at work to make the example shorter. In a real modeling context, you would determine the width of the COVID window and the optimal smooth parameters using the same methods we will employ to create the hurricane term. dat[ , `:=`( t = .I, yr = year(ds), yrfrac = as.numeric(strftime(ds, &#39;%j&#39;)), covid = 0L, wkdy = factor( strftime(ds, &#39;%u&#39;), labels = c(&#39;M&#39;, &#39;T&#39;, &#39;W&#39; ,&#39;Tr&#39;, &#39;F&#39;, &#39;Sat&#39;, &#39;Sun&#39;) ) ) ][ , yrfrac := yrfrac / max(yrfrac), by = .(yr) ][ .(seq(from = as.Date(&#39;2020-3-1&#39;), length.out = 122, by = 1)), covid := .I, on = &#39;ds&#39; ] hols_st &lt;- fread( file = file.path(&#39;data&#39;, &#39;hols_st.csv&#39;), colClasses = c(ds = &#39;Date&#39;) ) dat &lt;- hols_st[ dat, on = &#39;ds&#39; ][ is.na(hol), hol := &#39;none&#39; ] setkey(dat, ds) 3.3.2 Initial Model To start with, we can fit an initial model thats basically the same as the forecasting model we were working with in the prior chapter. I like to account for runout by setting more knot points for the trend component to allow it to capture the sharp drop in claims. ts &lt;- dat[ mday(ds) == 1 &amp; (month(ds) == 1 | ds &gt; as.Date(&#39;2021-6-30&#39;)), t ] m_init &lt;- gam( y ~ s(t) + s(yrfrac) + s(covid, k = 122 / 7) + wkdy + hol, family = gaussian(link = &#39;log&#39;), data = dat, knots = list(t = ts) ) plot(m_init, pages = 1) 3.3.3 Deriving the Holiday List My method for figuring out a good list of holidays to use is pretty simple: just examine residuals until there arent any holiday effects in the days with the largest prediction error. dat[ , yhat := as.vector(predict(m_init, type = &#39;response&#39;)) ][ , err := y - yhat ] dat[order(-abs(err)), .(ds, hol, err)][1:20, ] ## ds hol err ## 1: 2021-02-16 none -35.27132 ## 2: 2021-02-17 none -34.25963 ## 3: 2021-02-15 Presidents Day -33.54259 ## 4: 2019-11-29 none -30.68251 ## 5: 2018-12-24 none -30.16144 ## 6: 2021-02-18 none -28.81933 ## 7: 2022-12-20 none -26.41082 ## 8: 2019-12-24 none -25.93548 ## 9: 2019-11-22 Thanksgiving - Day After 25.91863 ## 10: 2022-12-21 none -25.89014 ## 11: 2020-12-24 none -25.42490 ## 12: 2022-02-03 none -24.04580 ## 13: 2022-12-19 none -23.14676 ## 14: 2022-02-04 none -19.43623 ## 15: 2021-02-19 none -17.49539 ## 16: 2018-01-16 none -17.16050 ## 17: 2022-12-16 none -14.53009 ## 18: 2018-12-31 none -14.31273 ## 19: 2022-02-24 none -13.16629 ## 20: 2022-12-15 none -12.12472 Theres actually a pattern in the residuals that I forgot about: Winter Storm Uri, the unofficial name for the ice storms that hit Texas in 2021. Well add a term for that below. As for holidays, the main potential that jumps out at me right now is effects related to Christmas Eve, so lets examine that more closely. lbls &lt;- c(paste0(12, 16:31), paste0(&#39;01&#39;, 0, 1:9)) pdat &lt;- dat[ , .(day = strftime(ds, &#39;%m%d&#39;), yr = year(ds) + (month(ds) == 12), wkdy, err) ][ .(lbls), on = &#39;day&#39; ][ , `:=`( yr = factor(yr), day = factor(day, levels = lbls) ) ] ggplot(pdat) + geom_bar( aes(day, err, group = wkdy, fill = wkdy), stat = &#39;identity&#39; ) + facet_grid(yr ~.) + scale_discrete_optum(&#39;fill&#39;) + labs( title = &#39;Christmas and New Year\\&#39;s&#39;, x = element_blank(), y = &#39;Utilization&#39;, fill = &#39;Weekday&#39; ) + mytheme + theme_rotx Chistmas Eve and New Years Eve appear to be exhibiting what I call bridge effects: the days only seem to have slowdowns when they create a four-day weekend along with the main holiday. Independence Day also displayed a similar pattern. I will skip straight to the final holiday list, but you would keep iterating on fitting models and examining the residuals to add more holiday or other outlier terms. dat[, hol := NULL] hols &lt;- fread( file = file.path(&#39;data&#39;, &#39;hols.csv&#39;), colClasses = c(ds = &#39;character&#39;) )[ , ds := as.Date(ds) ] dat &lt;- hols[dat, on = &#39;ds&#39;] dat[is.na(hol), hol := &#39;none&#39;] dat[, hol := relevel(factor(hol), ref = &#39;none&#39;)] 3.3.4 Hurricane Effect Window The last thing we need to do with out initial model is figure out a good effect window to assume for the hurricane. I do this in a similar way to creating the holiday list, by examining initial model residuals around landfall. ggplot(dat) + geom_vline(xintercept = landfall, linetype = &#39;dashed&#39;) + geom_bar(aes(ds, err, fill = wkdy), stat = &#39;identity&#39;) + scale_discrete_optum(&#39;fill&#39;) + coord_cartesian(xlim = landfall + c(-21, 21)) + labs( title = &#39;Initial Model Residuals Near Landfall&#39;, x = element_blank(), y = element_blank(), fill = element_blank() ) + mytheme Like with holidays, Im going to skip to using the window I already derived at work. To do it on your own, you would iterate through fitting a model with a potential hurricane window, examining the estimated smooth to make sure its reasonable, and examining the prediction errors around landfall for that model. 3.3.5 Penultimate Model After the above process is done, we have a specification for all of the outliers and interventions we need for the penultimate, or next to last, model. dat[, uri := 0L] dat[ .(seq(from = as.Date(&#39;2021-2-14&#39;), length.out = 7, by = 1)), uri := .I, on = &#39;ds&#39; ] dat[, harvey := 0L] dat[ .(seq(from = landfall - 2, length.out = 21, by = 1)), harvey := .I, on = &#39;ds&#39; ] m_penult &lt;- gam( y ~ s(t) + s(yrfrac) + s(covid, k = 122 / 7) + s(uri, k = 7) + s(harvey, k = 21) + wkdy + hol, family = gaussian(link = &#39;log&#39;), data = dat, knots = list(t = ts) ) summary(m_penult) ## ## Family: gaussian ## Link function: log ## ## Formula: ## y ~ s(t) + s(yrfrac) + s(covid, k = 122/7) + s(uri, k = 7) + ## s(harvey, k = 21) + wkdy + hol ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.785078 0.003034 1247.446 &lt; 2e-16 *** ## wkdyT -0.033896 0.004243 -7.988 2.23e-15 *** ## wkdyW -0.065864 0.004336 -15.189 &lt; 2e-16 *** ## wkdyTr -0.070122 0.004346 -16.137 &lt; 2e-16 *** ## wkdyF -0.185418 0.004637 -39.984 &lt; 2e-16 *** ## wkdySat -1.835358 0.018280 -100.403 &lt; 2e-16 *** ## wkdySun -2.395776 0.031691 -75.599 &lt; 2e-16 *** ## holChristmas -2.390926 0.269866 -8.860 &lt; 2e-16 *** ## holChristmas - Day After -0.289921 0.042223 -6.866 8.61e-12 *** ## holChristmas Eve -1.191191 0.099507 -11.971 &lt; 2e-16 *** ## holIndependence Day -1.933519 0.159968 -12.087 &lt; 2e-16 *** ## holIndependence Day - Bridge Day -0.298177 0.056288 -5.297 1.30e-07 *** ## holLabor Day -2.264282 0.192997 -11.732 &lt; 2e-16 *** ## holMemorial Day -2.265312 0.209980 -10.788 &lt; 2e-16 *** ## holNew Year&#39;s -1.848559 0.138372 -13.359 &lt; 2e-16 *** ## holNew Year&#39;s - Day After -0.142470 0.036071 -3.950 8.08e-05 *** ## holNew Year&#39;s Eve -0.434925 0.048754 -8.921 &lt; 2e-16 *** ## holPresidents Day 0.012921 0.022767 0.568 0.57 ## holThanksgiving -2.770453 0.335150 -8.266 2.40e-16 *** ## holThanksgiving - Day After -0.958753 0.061687 -15.542 &lt; 2e-16 *** ## holThanksgiving - Day Before -0.177955 0.025493 -6.981 3.91e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(t) 8.979 9.000 129.33 &lt;2e-16 *** ## s(yrfrac) 8.673 8.969 117.04 &lt;2e-16 *** ## s(covid) 9.305 11.236 95.60 &lt;2e-16 *** ## s(uri) 3.058 3.675 52.66 &lt;2e-16 *** ## s(harvey) 4.310 5.250 10.56 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.983 Deviance explained = 98.3% ## GCV = 5.2655 Scale est. = 5.132 n = 2181 plot(m_penult, pages = 1) From a quick look at summary() and plot(), it looks like our model is on the right track. There are two things we need to test for and correct before we can make valid statistical deductions from the model, however. 3.3.6 Serial Autocorrelation of Errors Since we are working with time series data, observations are correlated through time. A regression model cannot have any serial autocorrelation of errors, however, because the standard errors of the estimated coefficients will be too small. We check for serial autocorrelation using ACF and PACF plots. res &lt;- residuals(m_penult, type = &#39;response&#39;) sig_color &lt;- colors$Visualization$Iris myacf &lt;- acf(res, plot = F) ci &lt;- qnorm(.975)/sqrt(myacf$n.used) pdat &lt;- with( myacf, data.table(lag = as.vector(lag), acf = as.vector(acf)) ) pdat[lag == 0, acf := 0] myacf &lt;- ggplot(pdat, aes(x = lag, y = acf)) + geom_hline(aes(yintercept = 0)) + geom_segment(mapping = aes(xend = lag, yend = 0)) + labs(x = &#39;Lag&#39;, y = &#39;ACF&#39;) + geom_hline( aes(yintercept = -ci), linetype = &#39;dashed&#39;, color = sig_color ) + geom_hline( aes(yintercept = ci), linetype = &#39;dashed&#39;, color = sig_color ) + mytheme mypacf &lt;- pacf(res, plot = F) ci &lt;- qnorm(.975)/sqrt(mypacf$n.used) pdat &lt;- with( mypacf, data.table(lag = as.vector(lag), acf = as.vector(acf)) ) pdat[lag == 0, acf := 0] mypacf &lt;- ggplot(pdat, aes(x = lag, y = acf)) + geom_hline(aes(yintercept = 0)) + geom_segment(mapping = aes(xend = lag, yend = 0)) + labs(x = &#39;Lag&#39;, y = &#39;PACF&#39;) + geom_hline( aes(yintercept = -ci), linetype = &#39;dashed&#39;, color = sig_color ) + geom_hline( aes(yintercept = ci), linetype = &#39;dashed&#39;, color = sig_color ) + mytheme grid.arrange( myacf, mypacf, ncol = 2 ) A valid model will have all of the vertical lines (very close to) within the confidence bands. 3.3.7 Heteroskedasticity A regression model must also exhibit homoskedasticity, or have constant error variance. Violating this assumption not only decreases the standard errors but can also bias coefficient estimates. The fitting algorithms used by regression models assume that every observation contributes an equal amount of information to coefficient estimation. Heteroskedasticity entails that the higher-variance regions in the training data contribute less information. The standard way to check for it is via a scale-location plot. wpr &lt;- residuals(m_penult, type = &#39;pearson&#39;) pdat &lt;- data.table( # sqrt standardized Pearson residuals sspr = sqrt(abs(wpr / sd(wpr))), # predictions (on scale of linear predictor) pred = predict(m_penult) ) ggplot(pdat, aes(pred, sspr)) + geom_point() + geom_smooth( formula = y ~ x, method = &#39;lm&#39;, color = colors$Visualization$Strawberry ) + labs( title = &#39;Scale-Location Plot&#39;, x = &#39;Linear Predictor&#39;, y = &#39;Sqrt Std Pearson Residuals&#39; ) + mytheme Heteroskedasticity manifests in a scale-location plot as deviations from a flat, straight red line. A quick way to correct for it is to weight training observations with the errors from the penultimate model. It would be more elegant to use a priori weights, but I havent yet come up with a good algorithm for calculating those for the general utilization dataset. 3.3.8 Final Model To complete model fitting, we re-fit the penultimate model with an error correlation model to correct for serial autocorrelation and weights to correct for heteroskedasticity. Unfortunately, the best way to correct for serial autocorrelation that I believe is currently possible is very labor-intensive: you have to build a correlation model by hand, eyeballing the coefficients using the ACF and PACF plots. In general, the lines on the ACF chart tell you the lag-n autocorrelation coefficient, and those on the PACF the moving average coefficient. They interact with each other in ways that are difficult to predict, however. I start with the lag 1 and multiples of 7 autoregressive coefficients and go from there. It is not important to get these estimates exactly right, you just need a good enough correlation model to account for correlated residuals. We use the gamm() function (short for generalized additive mixed model) to pass in a correlation model for the errors and supply a weights argument for heteroskedasticity. wts &lt;- sqrt(abs(residuals(m_penult, type = &#39;scaled.pearson&#39;))) The code to fit the model looks like this. p &lt;- c(.55, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, .07, 0, 0, 0, 0, 0, 0, .06) q &lt;- c(0, 0, 0, 0, 0, .08, 0, .1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -.05) m_final &lt;- gamm( y ~ s(t) + s(yrfrac) + s(covid, k = 122 / 7) + s(uri, k = 7) + s(harvey, k = 21) + wkdy + hol, family = gaussian(link = &#39;log&#39;), data = dat, knots = list(t = ts), correlation = corARMA( value = c(p, q), p = length(p), q = length(q), fixed = T ), weights = wts ) Fitting this model took many hours because of all of the modifications weve added, so Ive just cached the fitted model in the data subfolder of our Github repository. m_final &lt;- readRDS(file = file.path(&#39;data&#39;, &#39;m_final.RDS&#39;)) summary(m_final$gam) ## ## Family: gaussian ## Link function: log ## ## Formula: ## y ~ s(t) + s(yrfrac) + s(covid, k = 122/7) + s(uri, k = 7) + ## s(harvey, k = 21) + wkdy + hol ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.7684028 0.0065583 574.601 &lt; 2e-16 *** ## wkdyT -0.0427913 0.0053828 -7.950 3.00e-15 *** ## wkdyW -0.0814813 0.0072804 -11.192 &lt; 2e-16 *** ## wkdyTr -0.1010531 0.0079996 -12.632 &lt; 2e-16 *** ## wkdyF -0.2180938 0.0083694 -26.058 &lt; 2e-16 *** ## wkdySat -1.9861325 0.0530410 -37.445 &lt; 2e-16 *** ## wkdySun -2.5842627 0.1048317 -24.652 &lt; 2e-16 *** ## holChristmas -2.2011358 0.3429394 -6.418 1.69e-10 *** ## holChristmas - Day After -0.2236717 0.0487236 -4.591 4.68e-06 *** ## holChristmas Eve -1.1433660 0.1284091 -8.904 &lt; 2e-16 *** ## holIndependence Day -1.6876224 0.1300657 -12.975 &lt; 2e-16 *** ## holIndependence Day - Bridge Day -0.2197339 0.0810224 -2.712 0.00674 ** ## holLabor Day -2.3877992 0.3534898 -6.755 1.84e-11 *** ## holMemorial Day -2.4493897 0.5459640 -4.486 7.63e-06 *** ## holNew Year&#39;s -1.6572612 0.1295243 -12.795 &lt; 2e-16 *** ## holNew Year&#39;s - Day After 0.0193896 0.0741909 0.261 0.79385 ## holNew Year&#39;s Eve -0.3398861 0.0580711 -5.853 5.58e-09 *** ## holPresidents Day 0.0008035 0.0403463 0.020 0.98411 ## holThanksgiving -0.6035092 0.0947922 -6.367 2.36e-10 *** ## holThanksgiving - Day After -0.5728268 0.0373162 -15.351 &lt; 2e-16 *** ## holThanksgiving - Day Before -0.0548677 0.0249713 -2.197 0.02811 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(t) 8.874 8.874 40.629 &lt; 2e-16 *** ## s(yrfrac) 7.597 7.597 21.868 &lt; 2e-16 *** ## s(covid) 6.212 6.212 17.103 &lt; 2e-16 *** ## s(uri) 3.092 3.092 11.071 5.53e-07 *** ## s(harvey) 2.505 2.505 4.515 0.0321 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.923 ## Scale est. = 11.506 n = 2181 plot(m_final$gam, pages = 1) res &lt;- residuals(m_final$lme, type = &#39;normalized&#39;) myacf &lt;- acf(res, plot = F) ci &lt;- qnorm(.975)/sqrt(myacf$n.used) pdat &lt;- with( myacf, data.table(lag = as.vector(lag), acf = as.vector(acf)) ) pdat[lag == 0, acf := 0] myacf &lt;- ggplot(pdat, aes(x = lag, y = acf)) + geom_hline(aes(yintercept = 0)) + geom_segment(mapping = aes(xend = lag, yend = 0)) + labs(x = &#39;Lag&#39;, y = &#39;ACF&#39;) + geom_hline( aes(yintercept = -ci), linetype = &#39;dashed&#39;, color = sig_color ) + geom_hline( aes(yintercept = ci), linetype = &#39;dashed&#39;, color = sig_color ) + mytheme mypacf &lt;- pacf(res, plot = F) ci &lt;- qnorm(.975)/sqrt(mypacf$n.used) pdat &lt;- with( mypacf, data.table(lag = as.vector(lag), acf = as.vector(acf)) ) pdat[lag == 0, acf := 0] mypacf &lt;- ggplot(pdat, aes(x = lag, y = acf)) + geom_hline(aes(yintercept = 0)) + geom_segment(mapping = aes(xend = lag, yend = 0)) + labs(x = &#39;Lag&#39;, y = &#39;PACF&#39;) + geom_hline( aes(yintercept = -ci), linetype = &#39;dashed&#39;, color = sig_color ) + geom_hline( aes(yintercept = ci), linetype = &#39;dashed&#39;, color = sig_color ) + mytheme grid.arrange( myacf, mypacf, ncol = 2 ) wpr &lt;- residuals(m_final$gam, type = &#39;pearson&#39;) / wts pdat &lt;- data.table( # sqrt standardized Pearson residuals sspr = sqrt(abs(wpr / sd(wpr))), # predictions (on scale of linear predictor) pred = predict(m_penult) ) ggplot(pdat, aes(pred, sspr)) + geom_point() + geom_smooth( formula = y ~ x, method = &#39;lm&#39;, color = colors$Visualization$Strawberry ) + labs( title = &#39;Scale-Location Plot&#39;, x = &#39;Linear Predictor&#39;, y = &#39;Sqrt Std Pearson Residuals&#39; ) + mytheme We are using the parameters Ive already come up with, but in reality, we would cycle through examining these four outputs and tweaking model parameters until everything looks good. This is a time-consuming and sometimes frustrating process! At the end of all this, however, we have a mathematically-robust estimate of the utilization impact due to Hurricane Harvey. We can visualize the estimated smooth like this. st &lt;- dat[harvey == 1, ds] hdat &lt;- as.data.table( confint( m_final, parm = &#39;s(harvey)&#39;, type = &#39;confidence&#39; ) )[ , `:=`( ind = st + (dat[, max(harvey)] / .N ) * .I - 1, sig = upper &lt; 0 | lower &gt; 0 ) ] ggplot(hdat, mapping = aes(ind)) + geom_hline(yintercept = 1, linetype = &#39;dashed&#39;) + geom_ribbon( aes(ymin = exp(lower), ymax = exp(upper)), alpha = .2 ) + geom_line( aes(y = exp(est), group = 1, color = sig), linewidth = 2, show.legend = F ) + scale_discrete_optum(aesthetics = &#39;color&#39;) + labs( title = &#39;Estimated Hurricane Harvey Impact&#39;, subtitle = &#39;Statistically Significant Areas Colored Red&#39;, x = element_blank(), y = element_blank() ) + mytheme 3.4 Using the Model 3.4.1 Normalizing Experience for Forecasts Now that we have a model were satisfied with, we can use it to normalize experience for forecasting. When forecasting, you have to remove any one-time impacts in history, otherwise forecasts will be biased. For example, imagine we have to forecast 2021 utilization in late 2020. This is what raw historicals looked like at the time. pdat &lt;- dat[year(ds) &lt; 2022, .(utilk = sum(y) / 12), keyby = .(yr)] ggplot(pdat[yr &lt; 2021]) + geom_line(aes(yr, utilk)) + labs( title = &#39;Historical Utilization through 2020&#39;, x = element_blank(), y = &#39;Utilization/1000 Members&#39; ) + mytheme Its not a good idea to include 2020 experience in calculation. Its also not a good idea to use raw 2020 actuals as the baseline for calculating the forecast. Doing all this using the naive average yearly method that you find on the exams would give the following results. i &lt;- pdat[, mean(utilk / shift(utilk, n = 1, type = &#39;lag&#39;) - 1, na.rm = T)] ggplot(pdat, aes(yr, utilk)) + geom_line() + geom_point( data = data.table( yr = 2021, utilk = pdat[yr == 2020, utilk * (1 + i)] ), color = colors$Visualization$Violet ) + labs( title = &#39;Actual vs. Expected 2021 Utilization&#39;, x = element_blank(), y = &#39;Utilization/1000 Members&#39; ) + mytheme Using that forecast for pricing 2021 plans would be a disaster! A too-low baseline year will lead to forecasts that are much too low, and including the depressed year in the trend calculation will make the forecasted trend too low, as well. Rather than making the forecast using the raw historicals with the one-time effects included, we can use the model to derive normalizing factors for historical periods. 3.4.2 Deriving Normalizing Factors from GAM Outputs fctrs &lt;- as.data.table( confint( m_final, parm = &#39;s(harvey)&#39;, n = 22 ) )[ , .(ds = seq(from = landfall - 1, length.out = 22, by = 1), est, lower, upper) ][ !(ds &lt; landfall), ][ , sig := upper &lt; 0 | lower &gt; 0 ][ , fctr := exp(sig * est) ][ , .(ds, fctr) ] fctrs[] ## ds fctr ## 1: 2017-08-25 1.0000000 ## 2: 2017-08-26 1.0000000 ## 3: 2017-08-27 0.9104837 ## 4: 2017-08-28 0.8854882 ## 5: 2017-08-29 0.8656999 ## 6: 2017-08-30 0.8526135 ## 7: 2017-08-31 0.8472624 ## 8: 2017-09-01 0.8489752 ## 9: 2017-09-02 0.8563584 ## 10: 2017-09-03 0.8680996 ## 11: 2017-09-04 1.0000000 ## 12: 2017-09-05 1.0000000 ## 13: 2017-09-06 1.0000000 ## 14: 2017-09-07 1.0000000 ## 15: 2017-09-08 1.0000000 ## 16: 2017-09-09 1.0000000 ## 17: 2017-09-10 1.0000000 ## 18: 2017-09-11 1.0000000 ## 19: 2017-09-12 1.0000000 ## 20: 2017-09-13 1.0000000 ## 21: 2017-09-14 1.0000000 ## ds fctr In practice, we probably want to allow the estimated factor to grade to 1 from the first and last days that are statistically significant, but that is just an exercise in arithmetic that you can do on your own if youre curious. ggplot(fctrs[fctr &lt; 1, ]) + geom_hline(yintercept = 1, linetype = &#39;dashed&#39;) + geom_bar( aes(ds, fctr), stat = &#39;identity&#39;, fill = colors$Visualization$Lagoon ) + labs( title = &#39;Estimated Hurricane Harvey Factors&#39;, x = element_blank(), y = &#39;Factor&#39; ) + mytheme Finally, we arrive at a normalizing factor for 2017Q3 historicals by taking an average of our final factors over that time period. fnl_fctr &lt;- dat[year(ds) == 2017 &amp; month(ds) %in% c(7, 8, 9), .(ds)] fnl_fctr &lt;- fctrs[fnl_fctr, on = &#39;ds&#39;] fnl_fctr[is.na(fctr), fctr := 1] fnl_fctr[, mean(fctr)] ## [1] 0.9884237 Around 112 basis points may seem unsatisfying after all that work, but professionals have to track impacts down below ten basis points, and Ive been on projects that were more work, less statistically sound, and had less impact than we see here. Thats the life of a forecasting actuaryif you like it, you love it; if not, maybe look into another field of actuarial science! "],["valuation-of-care-interventions.html", "Chapter 4 Valuation of Care Interventions 4.1 Introduction 4.2 Data 4.3 Model Fitting 4.4 Using the Model", " Chapter 4 Valuation of Care Interventions 4.1 Introduction Managed Care is an umbrella term for a wide variety of ways that health insurers participate in the planning and delivery of care. One part of this spectrum is the development of specialized reimbursement strategies meant to incentivize providers to deliver care more efficiently. This is broadly known as Value-based Care. Some examples are: Capitation - a payer and provider agree on a flat monthly rate for each of the payers members that is treated by the provider. The payer is thereby disincentivized from overutilization, since they wont be reimbursed for services over and above the average amount assumed in the contracted rate. Bundled Payments - a payer and provider agree on a single payment for a set of services. This is common in maternity claims, for example. Similar to capitation, the payer wont be reimbursed more if a member uses more services than the average assumed in the contract, but the payer will keep the surplus if they are able to treat the member using fewer. CMS ACO Risk Sharing Model - The overall cost per member per month of an ACO is compared against a baseline. Both the baseline and actual values are risk-adjusted in order to judge the ACO solely on efficiency. Depending on the specific model, the ACO gets to keep a proportion of any surplus and must pay CMS for any deficit. This is an extremely important subject for a health actuary to be knowledgeable about. There is broad consensus that the future of health care is value-based, and it is a huge competitive advantage for an insurer to be able to bend the trend of the cost of care in this way. 4.2 Data 4.2.1 Processed Dataset Data preparation is an even more important step than usual in workflows like this. Unfortunately, it involves a lot of protected health information, so we have to start with the processed, anonymized dataset. dat &lt;- fread(file = file.path(&#39;data&#39;, &#39;intv_data.csv&#39;)) fctrs &lt;- c(&#39;gdr&#39;, &#39;prdct&#39;, &#39;cdhp&#39;, &#39;funding&#39;, &#39;state&#39;, &#39;covid&#39;, &#39;intv&#39;) for(fctr in fctrs) dat[, (fctr) := factor(get(fctr))] dat[] ## yr mth scrpts gdr prdct cdhp funding state covid rs intv dur ## 1: 2020 9 1 1 1 1 1 2 FALSE 6.9058 FALSE -17 ## 2: 2020 10 1 1 1 1 1 2 FALSE 6.9058 FALSE -16 ## 3: 2020 11 0 1 1 1 1 2 FALSE 6.9058 FALSE -15 ## 4: 2020 12 0 1 1 1 1 2 FALSE 6.9058 FALSE -14 ## 5: 2021 1 0 1 1 1 1 2 FALSE 6.9058 FALSE -13 ## --- ## 45488: 2022 3 12 2 1 1 1 2 FALSE 2.6091 FALSE 0 ## 45489: 2021 12 3 1 1 2 1 2 FALSE 5.0894 FALSE -2 ## 45490: 2022 1 3 1 1 2 1 2 FALSE 5.0894 FALSE -1 ## 45491: 2022 2 5 1 1 2 1 2 FALSE 5.0894 FALSE 0 ## 45492: 2022 3 2 1 1 2 1 2 FALSE 5.0894 TRUE 1 This dataset is a combination of items from many sources, representing a significant analytic challenge. Here is a data dictionary summarizing the elements of this final table, along with some info about how it was processed into this final state. yr - Year mth - Month scrpts - Number of scripts filled by the member that month. Taken from claims data. gdr - Members gender code. Taken from membership data. prdct - Product code, like PPO, HMO, etc. Taken from membership data. cdhp - Indicator for whether or not the product is a high-deductible health plan. Taken from membership data. funding - Funding type for the product, fully-insured or advisory services only. Taken from membership data. state - Indicator for the state the member resides in. Taken from membership data. covid - Indicator for whether or not the current month is 202003. Remember, the main COVID impact to the Rx line of business is a spike in utilization at the beginning of the pandemic as companies allowed members to stockpile medications. rs - Risk score for the individual. These are the output of a proprietary Optum model meant to track the costliness of a member. Note that these are only calculated quarterly, so the last known value is carried forward. intv - Indicator for whether or not the intervention has taken place. Calculated by joining data from operations tracking files to claims and membership data. dur - The number of months since the intervention has occurred. To summarize, then, we have four data sources that needed to be processed. Claims - As a mature insurer, UHC has a strictly-structured claims database that serves as the source of truth for all clinical and financial operations around the company. Individuals are assigned a unique ID to facilitate joins. Membership - The same goes for membership and eligibility data at UHC. Risk Scores - These are maintained separately from claims and membership under a different product organization. Unfortunately, this data source uses a different unique ID than the claims and membership data source. Operations tracking files - a series of Excel workbooks maintained by the team actually administering the care intervention. These files are, of course, ignorant of the claims, membership, and risk score unique IDs, and are instead organized by member name. As a further complication, all of these names are input manually, so there is a risk of a members name being misspelled. The bulk of the initial work, therefore, was mapping member names to data source unique IDs. 4.2.2 Visualizations Lets take a look at what the time series of scripts looks like. pdat &lt;- dat[ , .(scrpts = sum(scrpts)), keyby = .(yrmo = paste0(yr, ifelse(mth &lt; 10, 0, &#39;&#39;), mth)) ] ggplot(pdat) + geom_line(aes(factor(yrmo), scrpts, group = 1)) + scale_y_continuous(labels = comma) + labs(x = element_blank(), y = &#39;Utilization&#39;) + mytheme + theme_rotx exposure &lt;- dat[ , .N, keyby = .(dur) ] ggplot(exposure) + geom_line(aes(dur, N)) + scale_y_continuous(labels = comma) + labs(y = &#39;Observations&#39;, x = &#39;Months Since Intervention&#39;) + mytheme 4.3 Model Fitting For this model, I made use of the random effects spline basis. If you scroll almost all the way down to the bottom of the output of ?smooth.terms, you will find out that there is a way of constructing a smooth term that yields the equivalent of a random effects term from mixed models. A random effects model treats the estimated coefficients as realizations of a random variable. This is preferable in many circumstances, one of which being those we have here, where we have a small sample of a much larger population. Its not just unlikely that the mean pharmacy quantity consumed by this population is anything like the true mean of the entire UHC book of business, much less the US as a whole: we know this beforehand because we are taking a sample of some of the most expensive members that UHC covers. Being able to use a model form that takes into account the bias in our sample will improve the ability of the model to generate useful, generalizable insights. The code to fit it is found in the following chunk. cl &lt;- makePSOCKcluster(detectCores()) m &lt;- bam( scrpts ~ s(yr, k = 3) + s(mth, k = 4) + covid + s(rs, bs = &#39;cs&#39;) + s(gdr, bs = &#39;re&#39;) + s(prdct, bs = &#39;re&#39;) + s(state, bs = &#39;re&#39;) + s(cdhp, bs = &#39;re&#39;) + s(funding, bs = &#39;re&#39;) + intv, family = nb(), data = dat[ exposure[N &gt; 99, ], on = &#39;dur&#39; ], cluster = cl ) stopCluster(cl) saveRDS(m, file = file.path(&#39;data&#39;, &#39;m_intv.RDS&#39;)) Again, I have cached the fitted model in our Github repo for ease of use. m &lt;- readRDS(file = file.path(&#39;data&#39;, &#39;m_intv.RDS&#39;)) summary(m) ## ## Family: Negative Binomial(1.928) ## Link function: log ## ## Formula: ## scrpts ~ s(yr, k = 3) + s(mth, k = 4) + covid + s(rs, bs = &quot;cs&quot;) + ## s(gdr, bs = &quot;re&quot;) + s(prdct, bs = &quot;re&quot;) + s(state, bs = &quot;re&quot;) + ## s(cdhp, bs = &quot;re&quot;) + s(funding, bs = &quot;re&quot;) + intv ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.84074 0.10203 8.240 &lt; 2e-16 *** ## covidTRUE 0.16487 0.02937 5.614 1.99e-08 *** ## intvTRUE -0.05447 0.01383 -3.937 8.25e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(yr) 1.6936 1.905 122.93 &lt; 2e-16 *** ## s(mth) 2.6623 2.913 33.30 &lt; 2e-16 *** ## s(rs) 8.7355 9.000 1860.94 &lt; 2e-16 *** ## s(gdr) 0.9861 1.000 99.85 &lt; 2e-16 *** ## s(prdct) 2.8744 3.000 364.05 &lt; 2e-16 *** ## s(state) 2.9337 3.000 308.15 &lt; 2e-16 *** ## s(cdhp) 0.9933 1.000 948.65 &lt; 2e-16 *** ## s(funding) 0.9476 1.000 454.82 1.25e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.203 Deviance explained = 20.5% ## fREML = 64839 Scale est. = 1 n = 45236 An \\(R^2\\) of ~20% is actually quite good for monthly, individual-level claims data. Like I said earlier, healthcare claims are just one of those subject areas that exhibit very high variance. plot(m, select = 1) plot(m, select = 2) plot(m, select = 3) 4.4 Using the Model 4.4.1 Use Cases for Valuation of Care Interventions By convention, VBC interventions are valued using a metric called ROI. It has a slightly different definition than in corporate finance. Here, it is simply the present value of expected savings divided by the present value of expected costs. An intervention needs an ROI greater than 100% to generate net savings. Once calculated, this metric can be used to price plans that include the intervention, negotiate with providers to share the expected savings, request funding to build up the organizational capabilities in order to administer it, etc. 4.4.2 Estimating Savings There are a few different ways to estimate the future savings that an intervention will generate. Since we have a model describing the effect of the intervention on a utilization metric, we would have to use monetization of utilization impact. To get to a per member per month savings for participants in the program, an assumption around cost per unit of utilization is applied to the estimated utilization impact. Deciding on this assumption is tricky for a number of reasons. First and foremost, the savings estimate is very sensitive to it, so a very precise estimate is needed. Second, unit cost is likely to be correlated with utilization. On the one hand, this means that a global average of all members will understate savings. On the other hand, the experienced unit cost for participants in the program may include some subtle upward biases that shouldnt be carried forward. Third, you need to extend the scope of your analysis to future plan design and benefit changes in addition to all the data youve had to wrangle so far. You cant take credit for savings for a block of business that will move to a capitated arrangement, for example, because utilization will no longer affect the insurers costs after that change. 4.4.3 A Simple Example As a simple example, I will demonstrate how to forecast the impact of this intervention on utilization trend for the next policy year. Lets assume we currently cover 10,000 members, all of whom will have coverage the entire next year. The next assumption we will need is called penetration, or the percentage of members that will receive the intervention. Depending on the intervention, this can be linked to the assumed morbidity of the insured pool, the distribution of members ages, etc. Finally, we need prior year utilization and baseline utilization trend assumptions. The calculations are demonstrated below. mbrs &lt;- 10000 pen &lt;- .03 ann_util_py &lt;- 25 trnd &lt;- .05 impct &lt;- 1 - unname(exp(coef(m)[&#39;intvTRUE&#39;])) actual_scrpts &lt;- mbrs * ann_util_py exp_scrpts &lt;- actual_scrpts * (1 + trnd) avoided_scrpts &lt;- mbrs * pen * ann_util_py * (1 + trnd) * impct fnl_trnd &lt;- (exp_scrpts - avoided_scrpts) / actual_scrpts - 1 trnd_impct &lt;- trnd - fnl_trnd trnd_impct ## [1] 0.001669789 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
